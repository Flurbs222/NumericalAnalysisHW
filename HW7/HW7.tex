\documentclass[12pt]{article}
\usepackage{Environments}
\usepackage{Packages}
\usepackage{listings}
\renewcommand{\qedsymbol}{\ensuremath{\blacksquare}}
\title{}
\author{Josh Morales}
\date{\today}
\setlength{\headheight}{15pt}
\begin{document}
\pagestyle{fancy}
\fancyhead[L]{Numerical Analysis HW 7}
\fancyhead[R]{Coyne, Dedvukaj, Gao, Karabushin, Lin, Morales}
\begin{center}
\textbf{\Large Homework 7} \\
\text{Due date}: \today
\end{center}
\begin{enumerate}[leftmargin=0em]
    %Question 1
    \item
    Note that 
    \[{||A||}_{\infty} = \max\{|3|+|2|+|4|,|2|+|0|+|2|, |4|+|2|+|3|\}= \max\{9,4,9\} = 9.\]
    Furthermore, we have that
    \[{||A||}_{1} = \max\{|3|+|2|+|4|, |2|+|0|+|2|, |4|+|2|+|3|\}= \max\{9,4,9\} = 9.\]
    Finally, we compute the eigenvalues of $A$ in the following way.
    \[\det(A-\lambda I)= 
    \det
    \begin{pmatrix}
        3-\lambda & 2 & 4\\
        2 & -\lambda & 2\\
        4 & 2 & 3-\lambda
    \end{pmatrix} = (3-\lambda)(-\lambda(3-\lambda)-4)-2(2(3-\lambda)-8)+4(4+4\lambda)\]
    \[= -\lambda^3+6\lambda^2+15\lambda+8=-{(\lambda+1)}^2(\lambda-8).\]
    Therefore, the eigenvalues are given by the roots of $-{(\lambda+1)}^2(\lambda-8)$, which are $\lambda=-1$ and $\lambda=8$. 
    Since $A$ is symmetric, we have that
    \[{||A||}_{2} = \rho(A) = \max\{|8|, |-1|\} = 8.\]


    %Question 2
    \item 
    \begin{enumerate}[leftmargin=!]
        %2a
        \item 
        \begin{proof}
            Let $M_1 = \max_{||x||\neq 0} \frac{||Ax||}{||x||}$ and $M_2 = ||A|| = \max_{||x|| = 1} ||Ax||$. It suffices to show that $M_1=M_2$. Note that if $||x||\neq 0$, then by properties of norms, we have
            \[\frac{||Ax||}{||x||}= \left|\left|A\frac{x}{||x||}\right|\right|.\]
            Furthermore\footnote{It should be noted that $\frac{x}{||x||}$ is intended to mean $\frac{1}{||x||}x$, to agree with multiplying x by a scalar.}, since $\left|\left|\frac{x}{||x||}\right|\right|= \frac{1}{||x||}||x|| = 1$
            we must have that $M_1 \leq M_2$ by definition of $M_2$. Furthermore, if we fix an arbitrary vector, $x$, such that $||x||=1$. Then we must have
            \[||Ax|| = \left|\left|A\frac{x}{1}\right|\right|=\left|\left|A\frac{x}{||x||}\right|\right|=\frac{||Ax||}{||x||}\]
            by definition of $M_1$, we must have that $M_2\leq M_1$, and therefore, $M_1=M_2$, which was the desired result.
        \end{proof}
        
        %2b
        \item

        \begin{proof}
            Note that by part $(a)$, we have that for any vector $x\neq 0$,
            \[||A|| = \max_{y\neq 0}\frac{||Ay||}{||y||} \geq \frac{||Ax||}{||x||}.\]
            Multiplying by $||x||$ gives that $||Ax||\leq ||A||||x||$ for all $x\neq 0$. Furthermore, note that the inequality also holds when $x=0$, since $Ax=0$. Therefore,
            \[||Ax||\leq ||A||||x||\]
            for all $x$. We then have that
            \[||AB|| = \max_{||x||=1||}||ABx||  \leq \max_{||x||=1}||A||||Bx||=||A||\max_{||x||=1}||Bx||= ||A||||B||\]
            which is the desired result.
        \end{proof}
        
        %2c
        \item 
        \begin{proof}
            By repetedly applying part $(b)$, we have that for all $k\in \NN$ 
            \[||A^k||= ||A\cdot A^{k-1}||\leq ||A||||A^{k-1}|| =||A||||A\cdot A^{k-2}|| \leq {||A||}^2||A^{k-2}|| \leq \ldots ||A||^{k}.\]
        \end{proof}
        
        %2d
        \item 
        \begin{proof}
            Note that by part $(b)$, we have
            \[||A||||A^{-1}|| \geq ||AA^{-1}|| = ||I|| = \max_{||x||=1}||Ix|| = \max_{||x||=1}||x|| = 1\]
            which is the desired result.
        \end{proof}
    \end{enumerate}

    %Question 3
    \item 
    \begin{proof}
	    
	\textbf{Step 1.} $\|A\|_1 \leq \max\limits_{1 \leq j \leq n} \sum\limits_{i=1}^{n} |a_{ij}|.$
	
	Fix $\mathbf{x}$ with $\|\mathbf{x}\|_1 = 1$. \quad ($\Rightarrow \sum\limits_{j=1}^{n} |x_j| = 1$)
	
	\begin{align*}
		\|A\mathbf{x}\|_1 & = \sum\limits_{i=1}^{n} \left| \sum\limits_{j=1}^{n} a_{ij} x_j \right| \\
		                  & \leq \sum\limits_{i=1}^{n} \sum\limits_{j=1}^{n} |a_{ij} x_j|           \\
		                  & \leq \sum\limits_{j=1}^{n} |x_j| \sum\limits_{i=1}^{n} |a_{ij}|         \\
		                  & \leq \max\limits_{1 \leq j \leq n} \sum\limits_{i=1}^{n} |a_{ij}|.      
	\end{align*}
	
	Taking the maximum over all $\mathbf{x}$ with $\|\mathbf{x}\|_1 = 1$, we get:
	\[
		\|A\|_1 = \max_{\|\mathbf{x}\|_1=1} \|A\mathbf{x}\|_1 \leq \max_{1 \leq j \leq n} \sum_{i=1}^{n} |a_{ij}|.
	\]
	
	\textbf{Step 2.} $\|A\|_1 \geq \max\limits_{1 \leq j \leq n} \sum\limits_{i=1}^{n} |a_{ij}|.$
	
	There exists some $p$ such that:
	\[
		\sum\limits_{i=1}^{n} |a_{ip}| = \max\limits_{1 \leq j \leq n} \sum\limits_{i=1}^{n} |a_{ij}|.
	\]
	
	Define $\mathbf{x} = [x_j]$ as:
	\[
		x_j = \begin{cases} 1 & j = p, \\ 0 & \text{else}
		\end{cases}\]
		
		($\|\mathbf{x}\|_1 = 1.$ and $a_{ij} x_j = |a_{ip}|$.)
		
		\begin{align*}
			\|A\mathbf{x}\|_1 & = \sum\limits_{i=1}^{n} \left| \sum\limits_{j=1}^{n} a_{ij} x_j \right| \\
			                  & = \sum\limits_{i=1}^{n} |a_{ip}|                                        \\
			                  & = \max\limits_{1 \leq j \leq n} \sum\limits_{i=1}^{n} |a_{ij}|.         
		\end{align*}
		
		Thus,
		\[
			\|A\|_1 = \max_{\|\mathbf{x}\|_1=1} \|A\mathbf{x}\|_1 \geq \max_{1 \leq j \leq n} \sum_{i=1}^{n} |a_{ij}|.
		\]
		
		Since both inequalities hold, we conclude:
		\[
			\|A\|_1 = \max\limits_{1 \leq j \leq n} \sum\limits_{i=1}^{n} |a_{ij}|.
		\]
		    
	\end{proof}
 
    %Question 4
    \item 
    For the Gauss-Seidel method, we get the following after each iteration:

    Iteration 1: $x[0] = 0.5000000, x[1] = 2.833333, x[2] = -1.083333$

    Iteration 2: $x[0] = 1.916667, x[1] = 2.944444, x[2] = -1.027778$

    Iteration 3: $x[0] = 1.972222, x[1] = 2.981481, x[2] = -1.009259$

    Iteration 4: $x[0] = 1.990741, x[1] = 2.993827, x[2] = -1.003086$

    Iteration 5: $x[0] = 1.996914, x[2] = 2.997942, x[3] = -1.001029$.

    Leaving the final solution as $[1.996914, 2.997924, -1.001029]$

    Here is the code that was used: 
    \begin{lstlisting}
        import numpy as np

        def gauss_seidel(A, b, x0=None, iterations=5):
            n = len(b)
            x = np.zeros(n) if x0 is None else np.array(x0, dtype=float)
            
            for iteration in range(1, iterations + 1):
                print(f"Iteration {iteration}:")
                for i in range(n):
                    sum1 = sum(A[i][j] * x[j] for j in range(i))
                    sum2 = sum(A[i][j] * x[j] for j in range(i + 1, n))
                    x[i] = (b[i] - sum1 - sum2) / A[i][i]
                    print(f"x[{i}] = {x[i]:.6f}")
                print()
            
            return x
            
        A = np.array([[2, -1, 0], [-1, 3, -1], [0, -1, 2]], dtype=float)
        b = np.array([1, 8, -5], dtype=float)
        x0 = [0, 0, 0]
        
        solution = gauss_seidel(A, b, x0, iterations=5)
        print("Final solution after 5 iterations:", solution)
    \end{lstlisting}

    For the Jacobi method, we got the following:


    Iteration 1: $x[0] = 0.500000, x[1] = 2.666667, x[2] = -2.5000000$

    Iteration 2: $x[0] = 1.833333, x[1] = 2.000000, x[2] = -1.166667$

    Iteration 3: $x[0] = 1.500000, x[1] = 2.888889, x[2] = -1.500000$

    Iteration 4: $x[0] = 1.944444, x[1] = 2.666667, x[2] = -1.055556$

    Iteration 5: $x[0] = 1.833333, x[1] = 2.962963, x[2] = -1.166667$

    Leaving final solution $[1.833333, 2.962963, -1.166667]$

    \begin{lstlisting}
    import numpy as np

        def jacobi_method(A, b, x0=None, iterations=5):
        n = len(b)
        x = np.zeros(n) if x0 is None else np.array(x0, dtype=float)
        x_new = np.copy(x)
    
        for iteration in range(1, iterations + 1):
            print(f"Iteration {iteration}:")
            for i in range(n):
                sum1 = sum(A[i][j] * x[j] for j in range(n) if j != i)
                x_new[i] = (b[i] - sum1) / A[i][i]
                print(f"x[{i}] = {x_new[i]:.6f}")
            x[:] = x_new 
            print()
    
        return x

    A = np.array([[2, -1, 0], [-1, 3, -1], [0, -1, 2]], dtype=float)
    b = np.array([1, 8, -5], dtype=float)
    x0 = [0, 0, 0]

    solution = jacobi_method(A, b, x0, iterations=5)
    print("Final solution after 5 iterations:", solution)
    \end{lstlisting}

    %Question 5
    \item 
    Let $T=I_{2\times 2}$, the $2\times 2$ identity matrix. Clearly, we have that
    \[{||T||}_{\infty} = \max\{1,1\} = 1.\]
    Furthermore, if we let \[\mathbf{c} = 
    \begin{pmatrix}
        1\\
        1
    \end{pmatrix},\]
    then starting with the initial condition $\mathbf{x}^{(0)} = \mathbf{0}$ gives 
    \[\mathbf{x}^{(k+1)}= I\mathbf{x}^{(k)}+\mathbf{c} = \mathbf{x}^{(k)}+\mathbf{c}.\]
    Therefore, we have
    \[\mathbf{x}^{(k)}= \mathbf{x}^{(k-1)}+\mathbf{c} = \mathbf{x}^{(k-2)}+2\mathbf{c} = \cdots = \mathbf{x}^{(0)}+k\mathbf{c} = k\mathbf{c}= 
    \begin{pmatrix}
        k\\
        k
    \end{pmatrix}.\]
    Therefore it suffices to show that ${||k\mathbf{c}||}_{\infty} \xrightarrow{k\to \infty} \infty$. Clearly, we have that
    \[{||k\mathbf{c}||}_{\infty} = \max\{|k|,|k|\} = k \rightarrow \infty\]
    as $k\rightarrow \infty$. Therefore, we are done.


    %Question 6
    \item 
    \begin{proof}
        First, assume that $A$ is strictly diagonally dominant. Namely, suppose that
        \[|a_{ii}| > \sum_{j\neq i}|a_{ij}|\]
        for all $1\leq i \leq n$. Then, we must have that
        \[{||D^{-1}(L+U)||}_{\infty} \leq {||D^{-1}||}_{\infty} {||L+U||}_{\infty} =  {||D^{-1}||}_{\infty}{||D-A||}_{\infty}\]
        where we used the fact that $A=D-(L+U)$. Furthermore, since $D=\diag(a_{ii})$\footnote{I am going to make this assumption, since the theorem is not true without it. For example, if $A=I$, then clearly $A$ is strictly diagonally dominant. However, if we let $D=\frac{1}{2}I$, then we must have $L+U= D-A=-\frac{1}{2}I$. This may be achieved with $L=U=-\frac{1}{4}I$. However, this means ${||D^{-1}(L+U)||}_{\infty}= {||2I\left(-\frac{1}{2}I\right)||}_{\infty} = {||-I||}_{\infty} = 1$ which is obviously not less than $1$. I specify this because we only assumed that $D$ was diagonal, $L$ is lower triangular, and $U$ is upper triangular. However, what I have written is a stronger condition.}, we must have that
        \[D^{-1} = \diag\left(\frac{1}{a_{ii}}\right).\]
        Therefore, since we have that
        \[L+U =
        \begin{cases}
            a_{ij} & \text{ if } i\neq j\\
            0 & \text{else}    
        \end{cases},\]
        we must have
        \[{[D^{-1}(L+U)]}_{ij} = 
        \begin{cases}
            \frac{a_{ij}}{a_{ii}} & \text{ if  } i\neq j\\
            0 & \text{else}
        \end{cases}.\]
        Therefore, we have that
        \[{||D^{-1}(L+U)||}_{\infty} = \max_{1\leq i \leq n}\sum_{j=1}^{n}\left|{[D^{-1}(L+U)]}_{ij}\right| = \max_{1\leq i \leq n}\sum_{j\neq i} \left|\frac{a_{ij}}{a_{ii}}\right|= \max_{1\leq i\leq n}\frac{1}{|a_{ii}|}\sum_{j\neq i}|a_{ij}|.\]
        Since $A$ is strictly diagonally dominant, we have that
        \[{||D^{-1}(L+U)||}_{\infty}= \max_{1\leq i\leq n}\frac{1}{|a_{ii}|}\sum_{j\neq i}|a_{ij}| < \max_{1\leq i \leq n}\frac{|a_{ii}|}{|a_{ii}|} = 1.\]
        Therefore, ${||D^{-1}(L+U)||}_{\infty}<1$.

        \bigskip 

        \noindent Suppose that ${||D^{-1}(L+U)||}_{\infty}<1$. As discussed before, we have that
        \[{||D^{-1}(L+U)||}_{\infty}= \max_{1\leq i\leq n}\frac{1}{|a_{ii}|}\sum_{j\neq i}|a_{ij}|.\]
        Therefore, for all $1\leq i \leq n$, we have that
        \[\frac{1}{|a_{ii}|}\sum_{j\neq i}|a_{ij}|\leq \max_{1\leq i\leq n}\frac{1}{|a_{ii}|}\sum_{j\neq i}|a_{ij}|<1.\]
        Thus, multiplying by $a_{ii}$ gives that for all $1\leq i\leq n$,
        \[\sum_{j\neq i}|a_{ij}|<|a_{ii}|.\]
        Therefore, $A$ is strictly diagonally dominant, which completes the proof.
    \end{proof}
\end{enumerate}
\end{document}
